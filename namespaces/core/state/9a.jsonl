{"_at":[1768455524887,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768448962846,1],"darin@book"],"claim":[[1768454823391,1],"darin@book"],"description":[[1768448962846,1],"darin@book"],"design":[[1768448962846,1],"darin@book"],"estimated_minutes":[[1768448962846,1],"darin@book"],"external_ref":[[1768448962846,1],"darin@book"],"labels":[[1768448962846,1],"darin@book"],"priority":[[1768448962846,1],"darin@book"],"source_repo":[[1768448962846,1],"darin@book"],"title":[[1768448962846,1],"darin@book"],"type":[[1768448962846,1],"darin@book"]},"acceptance_criteria":"- [ ] pending_events is bounded by size/bytes or removed entirely.\n- [ ] Overflow behavior is explicit (drop policy + metric/log).\n- [ ] Tests or assertions cover the cap behavior.\n\n**Files:** src/daemon/repl/manager.rs, src/daemon/repl/server.rs","assignee":"darin@book","assignee_at":[1768454823391,1],"assignee_expires":1768458423391,"closed_at":[1768455524887,1],"closed_by":"darin@book","created_at":[1768448962846,1],"created_by":"darin@book","description":"**Problem**\nBoth inbound and outbound replication loops buffer events before streaming is established, but the pending_events Vec has no size/byte bound. If handshake stalls or a peer stays non-streaming, this can grow without bound and consume memory.\n\n**Evidence**\n- src/daemon/repl/manager.rs: pending_events: Vec<BroadcastEvent> grows while !streaming.\n- src/daemon/repl/server.rs: pending_events: Vec<BroadcastEvent> grows while !streaming.\n\n**Why this hurts**\nA slow or stuck peer can cause unbounded memory growth in the daemon, which is brittle and hard to reason about.","design":"**Design**\nOption A: Replace pending_events with a bounded VecDeque that caps both event count and bytes (use limits.max_event_batch_events / max_event_batch_bytes or broadcaster hot-cache limits). Drop oldest events and record a metric when overflowing.\nOption B: Remove pending_events entirely and rely on hot_cache + WANT after handshake, avoiding buffering while not streaming.\n\nEither way, prevent unbounded growth and keep behavior deterministic.","id":"bd-6lv","labels":[],"priority":2,"status":"closed","title":"Replication pending_events queue is unbounded","type":"bug"}
{"_at":[1765677592520,0],"_by":"darin@dusk","_v":{"acceptance_criteria":[[1765676872178,0],"darin@dusk"],"claim":[[1765677055015,0],"darin@dusk"],"description":[[1765676872178,0],"darin@dusk"],"design":[[1765676872178,0],"darin@dusk"],"estimated_minutes":[[1765676872178,0],"darin@dusk"],"external_ref":[[1765676872178,0],"darin@dusk"],"labels":[[1765676872178,0],"darin@dusk"],"priority":[[1765676872178,0],"darin@dusk"],"source_repo":[[1765676872178,0],"darin@dusk"],"title":[[1765676872178,0],"darin@dusk"],"type":[[1765676872178,0],"darin@dusk"]},"assignee":"darin@dusk","assignee_at":[1765677055015,0],"assignee_expires":1765680655015,"closed_at":[1765677592520,0],"closed_by":"darin@dusk","created_at":[1765676872178,0],"created_by":"darin@dusk","description":"## What's Wrong\nflake.nix has doCheck=false to avoid test failures during nix build. This means `nix build` won't catch test regressions.\n\n## Where\n- flake.nix:42-43\n\n## Why It Matters\nUsers installing via nix won't get test validation. If tests break, nix users get broken builds.\n\n## Options\n1. Fix whatever causes tests to fail in nix sandbox (likely git/network related)\n2. Add a separate `nix flake check` that runs tests outside the build\n3. Accept the gap and rely on CI (current state)","id":"bd-97w","labels":[],"priority":2,"status":"closed","title":"Nix flake skips tests (doCheck=false) - CI gap","type":"chore"}
{"_at":[1768416234514,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768411803728,1],"darin@book"],"claim":[[1768416108511,1],"darin@book"],"description":[[1768411803728,1],"darin@book"],"design":[[1768411803728,1],"darin@book"],"estimated_minutes":[[1768411803728,1],"darin@book"],"external_ref":[[1768411803728,1],"darin@book"],"labels":[[1768411803728,1],"darin@book"],"priority":[[1768411803728,1],"darin@book"],"source_repo":[[1768411803728,1],"darin@book"],"title":[[1768411803728,1],"darin@book"],"type":[[1768411803728,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768416108511,1],"assignee_expires":1768419708511,"closed_at":[1768416234514,1],"closed_by":"darin@book","created_at":[1768411803728,1],"created_by":"darin@book","description":"**Problem**\n`tests/critical_path.rs` and `tests/migration.rs` share a single runtime/data dir for the entire test process via `test_runtime_dir()` (OnceLock + `/tmp/beads-test-runtime-<pid>`). That means all tests in both modules share the same XDG_RUNTIME_DIR/BD_DATA_DIR and the same daemon socket. There is no cleanup of that directory, and parallel test runs can cross-talk through the shared daemon/state. This undermines realtime phase test isolation and makes failures order-dependent.\n\nEvidence:\n- `tests/critical_path.rs` lines 14-28 define `test_runtime_dir()` with OnceLock and a fixed path.\n- `tests/migration.rs` lines 11-25 define the same pattern and reuse it for BD env.\n\n**Design**\n- Replace `test_runtime_dir()` with a per-test TempDir runtime (owned by the test fixture / TestRepo).\n- Make `TestRepo` carry `runtime_dir: TempDir` and `data_dir: PathBuf`, and update `bd()` to use those paths.\n- Reuse the shared shutdown helper from the daemon teardown bead so each runtime is cleaned up on Drop.\n- Avoid shared globals so tests can run in parallel without state collisions.\n\n**Acceptance**\n- [ ] `tests/critical_path.rs` and `tests/migration.rs` no longer use OnceLock-based runtime dirs.\n- [ ] Each test uses a unique TempDir for XDG_RUNTIME_DIR/BD_DATA_DIR and it is removed on Drop.\n- [ ] Tests can run with `--test-threads=N` without sharing a daemon/socket.\n\n**Files:**\n- tests/critical_path.rs\n- tests/migration.rs\n- tests/fixtures/daemon_runtime.rs (new, shared helper)","id":"bd-j9l","labels":[],"priority":1,"status":"closed","title":"tests: critical_path/migration share runtime dir","type":"bug"}
