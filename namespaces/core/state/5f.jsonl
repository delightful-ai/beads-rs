{"_at":[1768473798982,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768449542102,1],"darin@book"],"claim":[[1768472262322,1],"darin@book"],"description":[[1768449567786,1],"darin@book"],"design":[[1768449542102,1],"darin@book"],"estimated_minutes":[[1768449542102,1],"darin@book"],"external_ref":[[1768449542102,1],"darin@book"],"labels":[[1768449542102,1],"darin@book"],"priority":[[1768449542102,1],"darin@book"],"source_repo":[[1768449542102,1],"darin@book"],"title":[[1768449542102,1],"darin@book"],"type":[[1768449542102,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768472262322,1],"assignee_expires":1768475862322,"closed_at":[1768473798982,1],"closed_by":"darin@book","created_at":[1768449542102,1],"created_by":"darin@book","description":"**Problem**\nREALTIME_PLAN §16.2 specifies `admin.flush` to force fsync for a namespace (and optionally trigger a checkpoint). There is no admin flush handler or IPC/CLI surface; `src/daemon/admin.rs` only has status/metrics/doctor/scrub/reload/maintenance/rotate. This leaves no explicit manual durability/flush hook.\n\n**Design**\n- Add admin IPC op + CLI subcommand `bd admin flush [--namespace] [--checkpoint-now]` (namespace default core).\n- Implement daemon handler to call WAL fsync for the namespace and optionally enqueue checkpoint for affected groups.\n- Surface in `src/api` + renderers; include timing/durability info in response.\n\n**Acceptance**\n- [ ] IPC + CLI `admin flush` exists and returns success payload.\n- [ ] WAL fsync performed for requested namespace.\n- [ ] `--checkpoint-now` schedules a checkpoint for groups including that namespace.\n- [ ] Tests cover handler + response shape.\n\n**Files:** src/daemon/admin.rs, src/daemon/ipc.rs, src/cli/mod.rs, src/cli/render.rs, src/api","id":"bd-3m5.100","labels":[],"priority":2,"status":"closed","title":"admin.flush missing (fsync + optional checkpoint trigger)","type":"bug"}
{"_at":[1766124427491,0],"_by":"darin@dusk","_v":{"acceptance_criteria":[[1766122273851,0],"darin@dusk"],"claim":[[1766124427391,0],"darin@dusk"],"description":[[1766122273851,0],"darin@dusk"],"design":[[1766122273851,0],"darin@dusk"],"estimated_minutes":[[1766122273851,0],"darin@dusk"],"external_ref":[[1766122273851,0],"darin@dusk"],"labels":[[1766122273851,0],"darin@dusk"],"priority":[[1766122273851,0],"darin@dusk"],"source_repo":[[1766122273851,0],"darin@dusk"],"title":[[1766122273851,0],"darin@dusk"],"type":[[1766122273851,0],"darin@dusk"]},"assignee":"darin@dusk","assignee_at":[1766124427391,0],"assignee_expires":1766128027391,"closed_at":[1766124427491,0],"closed_by":"darin@dusk","created_at":[1766122273851,0],"created_by":"darin@dusk","description":"**Problem**\\nHLC relies on wall clock time. If a machine clock jumps backward or is far ahead, its stamps can dominate LWW merges and effectively overwrite other actors' updates. This can look like data loss.\\n\\n**Design**\\nPersist the last-seen WriteStamp (per-repo) and clamp local wall_ms to >= last_seen.wall_ms. If local time moves backwards, advance logical counter instead of wall time. Add skew detection (warn if now << last_seen or now >> last_seen by threshold). Consider storing max observed stamp in meta or WAL to survive restarts.\\n\\n**Acceptance**\\n- [ ] Stamps remain monotonic across restarts and backward clock jumps.\\n- [ ] Skew detection is surfaced (status/log).\\n- [ ] Tests simulate backward/forward jumps and verify monotonicity + merge ordering.\\n\\n**Files:** src/daemon/clock.rs, src/core/time.rs, src/daemon/core.rs","id":"bd-eqi","labels":[],"priority":1,"status":"closed","title":"Clock skew can break LWW ordering","type":"bug"}
{"_at":[1765781499657,0],"_by":"darin@dusk","_v":{"acceptance_criteria":[[1765744419494,0],"darin@book"],"claim":[[1765781360961,0],"darin@dusk"],"description":[[1765744419494,0],"darin@book"],"design":[[1765744419494,0],"darin@book"],"estimated_minutes":[[1765744419494,0],"darin@book"],"external_ref":[[1765744419494,0],"darin@book"],"labels":[[1765744419494,0],"darin@book"],"priority":[[1765744419494,0],"darin@book"],"source_repo":[[1765744419494,0],"darin@book"],"title":[[1765744419494,0],"darin@book"],"type":[[1765744419494,0],"darin@book"]},"assignee":"darin@dusk","assignee_at":[1765781360961,0],"assignee_expires":1765784960961,"closed_at":[1765781499657,0],"closed_by":"darin@dusk","created_at":[1765744419494,0],"created_by":"darin@book","description":"**Problem**\nCommon pattern of \"check exists, check not deleted, then unwrap later\":\n```rust\nif repo_state.state.get_live(id).is_none() {\n    if repo_state.state.get_tombstone(id).is_some() {\n        return Response::err(OpError::BeadDeleted(id.clone()));\n    }\n    return Response::err(OpError::NotFound(id.clone()));\n}\n// ... later ...\nlet bead = repo_state.state.get_live_mut(id).unwrap();\n```\n\n**Design**\nAdd a combined lookup that returns the appropriate error:\n```rust\npub enum LiveLookupError {\n    NotFound,\n    Deleted,\n}\n\nimpl CanonicalState {\n    pub fn require_live(&self, id: &BeadId) -> Result<&Bead, LiveLookupError> {\n        if let Some(b) = self.get_live(id) {\n            return Ok(b);\n        }\n        if self.get_tombstone(id).is_some() {\n            return Err(LiveLookupError::Deleted);\n        }\n        Err(LiveLookupError::NotFound)\n    }\n\n    pub fn require_live_mut(&mut self, id: &BeadId) -> Result<&mut Bead, LiveLookupError> {\n        // Same logic for mutable access\n    }\n}\n```\n\nDaemon maps `LiveLookupError` → `OpError::{NotFound, BeadDeleted}`.\n\n**Acceptance**\n- [ ] `LiveLookupError` enum in `src/core/state.rs`\n- [ ] `require_live` and `require_live_mut` methods on `CanonicalState`\n- [ ] Executor/query code uses these instead of check-then-unwrap pattern\n- [ ] Tests for NotFound vs Deleted cases\n\n**Files:** src/core/state.rs, src/daemon/executor.rs, src/daemon/query_executor.rs","id":"bd-ieo","labels":[],"priority":2,"status":"closed","title":"Add require_live helper to eliminate bead lookup boilerplate","type":"chore"}
