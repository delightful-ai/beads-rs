{"_at":[1768425885067,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768422079065,1],"darin@book"],"claim":[[1768425711572,1],"darin@book"],"description":[[1768422172484,1],"darin@book"],"design":[[1768422079065,1],"darin@book"],"estimated_minutes":[[1768422079065,1],"darin@book"],"external_ref":[[1768422079065,1],"darin@book"],"labels":[[1768422079065,1],"darin@book"],"priority":[[1768422079065,1],"darin@book"],"source_repo":[[1768422079065,1],"darin@book"],"title":[[1768422079065,1],"darin@book"],"type":[[1768422079065,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768425711572,1],"assignee_expires":1768429311572,"closed_at":[1768425885067,1],"closed_by":"darin@book","created_at":[1768422079065,1],"created_by":"darin@book","description":"Observed during benchmark: cargo test --test phase7_subscribe failed with phase7_subscribe_streams_events_in_order panicking at tests/phase7_subscribe.rs:170 (event). 18/19 passed. Likely flake or subscribe stream ordering/EOF issue; rerun with RUST_BACKTRACE=1 and add logging around StreamingClient::next_event/subscribe_stream.","id":"bd-2a5","labels":[],"priority":1,"status":"closed","title":"phase7_subscribe_streams_events_in_order failed during benchmark","type":"bug"}
{"_at":[1768461466235,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768444212747,1],"darin@book"],"claim":[[1768461230827,1],"darin@book"],"description":[[1768444212747,1],"darin@book"],"design":[[1768444212747,1],"darin@book"],"estimated_minutes":[[1768444212747,1],"darin@book"],"external_ref":[[1768444212747,1],"darin@book"],"labels":[[1768444212747,1],"darin@book"],"priority":[[1768444212747,1],"darin@book"],"source_repo":[[1768444212747,1],"darin@book"],"title":[[1768444212747,1],"darin@book"],"type":[[1768444212747,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768461230827,1],"assignee_expires":1768464830827,"closed_at":[1768461466235,1],"closed_by":"darin@book","created_at":[1768444212747,1],"created_by":"darin@book","description":"**Problem**\nREALTIME_PLAN §0.10 defines LocalFsync as fsync of the active segment file after each append. Current SegmentWriter::append uses File::sync_data, which may omit metadata durability (file length/mtime) on some platforms.\n\nEvidence:\n- src/daemon/wal/segment.rs:252–260 calls self.file.sync_data().\n\n**Why this violates plan**\nLocalFsync semantics require full fsync of the file to guarantee record durability.\n\n**Acceptance**\n- [ ] Use sync_all (fsync) for LocalFsync record durability.\n- [ ] Document/benchmark any performance impact and keep directory fsync only on rotation.","id":"bd-3m5.89","labels":[],"priority":2,"status":"closed","title":"WAL append uses sync_data instead of full fsync","type":"bug"}
