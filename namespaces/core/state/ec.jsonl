{"_at":[1768445990476,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768364140165,1],"darin@book"],"claim":[[1768443365145,1],"darin@book"],"description":[[1768364151661,1],"darin@book"],"design":[[1768364140165,1],"darin@book"],"estimated_minutes":[[1768364140165,1],"darin@book"],"external_ref":[[1768364140165,1],"darin@book"],"labels":[[1768364140165,1],"darin@book"],"priority":[[1768364140165,1],"darin@book"],"source_repo":[[1768364140165,1],"darin@book"],"title":[[1768364140165,1],"darin@book"],"type":[[1768364140165,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768443365145,1],"assignee_expires":1768446965145,"closed_at":[1768445990476,1],"closed_by":"darin@book","created_at":[1768364140165,1],"created_by":"darin@book","description":"**Problem**\nDurabilityCoordinator waits by sleeping in the daemon state thread, blocking other IPC/git handling during ReplicatedFsync.\n\n**Design**\nMove durability waits into async waiters managed by run_state_loop. After local apply, register a waiter with the response channel and poll ack status on a short tick or ack notifications; respond when satisfied or timeout without blocking the loop.\n\n**Acceptance**\n- [ ] ReplicatedFsync requests no longer block the state loop.\n- [ ] DurabilityTimeout still returns retryable error with receipt.\n- [ ] Tests cover waiter completion + timeout paths.\n\n**Files:** src/daemon/server.rs, src/daemon/core.rs, src/daemon/durability_coordinator.rs","id":"bd-1vu","labels":[],"priority":2,"status":"closed","title":"DurabilityCoordinator wait blocks daemon loop; refactor to async waiters","type":"bug"}
{"_at":[1768472249326,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768449581458,1],"darin@book"],"claim":[[1768471536792,1],"darin@book"],"description":[[1768449581458,1],"darin@book"],"design":[[1768449581458,1],"darin@book"],"estimated_minutes":[[1768449581458,1],"darin@book"],"external_ref":[[1768449581458,1],"darin@book"],"labels":[[1768449581458,1],"darin@book"],"priority":[[1768449581458,1],"darin@book"],"source_repo":[[1768449581458,1],"darin@book"],"title":[[1768449581458,1],"darin@book"],"type":[[1768449581458,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768471536792,1],"assignee_expires":1768475136792,"closed_at":[1768472249326,1],"closed_by":"darin@book","created_at":[1768449581458,1],"created_by":"darin@book","description":"**Problem**\nREALTIME_PLAN §13.4 requires sensitive store-local files to be 0600 (namespaces.toml, replicas.toml, wal.sqlite, meta.json) and WAL namespace dirs to be 0700 with symlink escape prevention. Current code enforces 0600 for store_meta and wal.sqlite, but `load_namespace_policies` + `load_replica_roster` read files without permission checks, and WAL namespace dirs are created with `create_dir_all` in `src/daemon/wal/segment.rs` without symlink checks or explicit 0700 perms.\n\n**Design**\n- Add a shared helper to enforce secure perms + reject symlinks for store-local config files.\n- Apply to `namespaces.toml` and `replicas.toml` read/write paths.\n- When creating `wal/<namespace>/` directories, set 0700 and reject symlink components (similar to store lock).\n\n**Acceptance**\n- [ ] namespaces.toml + replicas.toml are 0600 (or corrected on load) and symlinked paths are rejected.\n- [ ] wal/<namespace>/ directories are 0700 and symlink escapes are rejected.\n- [ ] Tests cover permission enforcement and symlink rejection.\n\n**Files:** src/daemon/store_runtime.rs, src/daemon/core.rs, src/daemon/wal/segment.rs, src/paths.rs","id":"bd-3m5.101","labels":[],"priority":2,"status":"closed","title":"enforce 0600/0700 perms for store policy files + WAL dirs","type":"bug"}
{"_at":[1768480674530,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768444258974,1],"darin@book"],"claim":[[1768480130779,1],"darin@book"],"description":[[1768444258974,1],"darin@book"],"design":[[1768444258974,1],"darin@book"],"estimated_minutes":[[1768444258974,1],"darin@book"],"external_ref":[[1768444258974,1],"darin@book"],"labels":[[1768444258974,1],"darin@book"],"priority":[[1768444258974,1],"darin@book"],"source_repo":[[1768444258974,1],"darin@book"],"title":[[1768444258974,1],"darin@book"],"type":[[1768444258974,1],"darin@book"]},"assignee":"darin@book","assignee_at":[1768480130779,1],"assignee_expires":1768483730779,"closed_at":[1768480674530,1],"closed_by":"darin@book","created_at":[1768444258974,1],"created_by":"darin@book","description":"**Problem**\nREALTIME_PLAN §3.1 says duplicate map keys in EventBody SHOULD be rejected. Current decode_event_body_map/other decoders accept duplicates silently (later key wins).\n\nEvidence:\n- src/core/event.rs decode_event_body_map loops over keys with no duplicate tracking.\n\n**Acceptance**\n- [ ] Track seen keys in EventBody/TxnDelta/HlcMax and return DecodeError::DuplicateKey (or similar) on duplicates.\n- [ ] Add tests for duplicate keys being rejected.","id":"bd-3m5.90","labels":[],"priority":3,"status":"closed","title":"EventBody decoder doesn't detect duplicate map keys","type":"bug"}
{"_at":[1765949974228,0],"_by":"darin@dusk","_v":{"acceptance_criteria":[[1765949734336,0],"darin@dusk"],"claim":[[1765949734336,0],"darin@dusk"],"description":[[1765949779370,0],"darin@dusk"],"design":[[1765949734336,0],"darin@dusk"],"estimated_minutes":[[1765949734336,0],"darin@dusk"],"external_ref":[[1765949734336,0],"darin@dusk"],"labels":[[1765949734336,0],"darin@dusk"],"priority":[[1765949734336,0],"darin@dusk"],"source_repo":[[1765949734336,0],"darin@dusk"],"title":[[1765949734336,0],"darin@dusk"],"type":[[1765949734336,0],"darin@dusk"]},"closed_at":[1765949974228,0],"closed_by":"darin@dusk","closed_reason":"Fixed: added Request::Refresh and notify daemon after migration","created_at":[1765949734336,0],"created_by":"darin@dusk","description":"**Problem**\nWhen migrating from beads-go to beads-rs in a PR branch, the sync doesn't work properly. Beads appear not to exist after migration.\n\n**Root Cause**\nThe `migrate from-go` command bypasses the daemon entirely - it directly writes to `refs/heads/beads/store` using `SyncProcess` (src/cli/commands/migrate.rs:71-74). But the daemon caches state in memory per-remote in `RepoState`. After migration completes:\n1. Git ref has the migrated state\n2. Daemon still has empty/old cached state\n3. CLI queries go through daemon, return stale data\n\n**Design**\nAdd a `Request::Refresh` IPC command that tells the daemon to reload state from the git ref for a given repo. The migrate command should call this after successful migration.\n\nAlternatively, migrate could go through the daemon IPC entirely, but that's more invasive.\n\n**Acceptance**\n- [ ] After `bd migrate from-go`, `bd list` shows the migrated beads\n- [ ] Works even if daemon was already running before migration\n- [ ] Test: migrate in a fresh clone, verify state is visible\n\n**Files:** src/daemon/ipc.rs, src/cli/commands/migrate.rs, src/daemon/core.rs","id":"bd-am6","labels":[],"priority":1,"status":"closed","title":"Sync fails when migrating from beads-go in PR branch with existing store","type":"bug"}
{"_at":[1767995408322,1],"_by":"darin@darinsmacstudio.lan","_v":{"acceptance_criteria":[[1765780960927,0],"darin@dusk"],"claim":[[1767995288667,1],"darin@darinsmacstudio.lan"],"description":[[1765780960927,0],"darin@dusk"],"design":[[1765780960927,0],"darin@dusk"],"estimated_minutes":[[1765780960927,0],"darin@dusk"],"external_ref":[[1765780960927,0],"darin@dusk"],"labels":[[1765780960927,0],"darin@dusk"],"priority":[[1765780960927,0],"darin@dusk"],"source_repo":[[1765780960927,0],"darin@dusk"],"title":[[1765780960927,0],"darin@dusk"],"type":[[1765780960927,0],"darin@dusk"]},"assignee":"darin@darinsmacstudio.lan","assignee_at":[1767995288667,1],"assignee_expires":1767998888667,"closed_at":[1767995408322,1],"closed_by":"darin@darinsmacstudio.lan","created_at":[1765780960927,0],"created_by":"darin@dusk","description":"Currently bd ready shows issues in some order but should be sorted by priority so that P0 issues appear first and P4 issues appear last. This helps agents pick the most important work first.","id":"bd-efx","labels":[],"priority":3,"status":"closed","title":"bd ready: sort by priority (P0 first, P4 last)","type":"feature"}
{"_at":[1768451885428,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768444184831,1],"darin@book"],"claim":[[1768450906998,1],"darin@book"],"description":[[1768444184831,1],"darin@book"],"design":[[1768444184831,1],"darin@book"],"estimated_minutes":[[1768444184831,1],"darin@book"],"external_ref":[[1768444184831,1],"darin@book"],"labels":[[1768444184831,1],"darin@book"],"priority":[[1768444184831,1],"darin@book"],"source_repo":[[1768444184831,1],"darin@book"],"title":[[1768444184831,1],"darin@book"],"type":[[1768444184831,1],"darin@book"]},"acceptance_criteria":"- [ ] Remote ingest never appends/indexes an event that would fail `apply_event`.\n- [ ] Apply failures map to a specific error code (not generic `internal_error`).\n- [ ] Tests cover a remote event that would currently fail apply and verify it is rejected before WAL append.","assignee":"darin@book","assignee_at":[1768450906998,1],"assignee_expires":1768454506998,"closed_at":[1768451885428,1],"closed_by":"darin@book","created_at":[1768444184831,1],"created_by":"darin@book","description":"**Problem**\nRemote ingest appends events to WAL + SQLite *before* calling `apply_event`. If `apply_event` fails (missing bead, note collision, invalid dep, etc.), the event is already durable and indexed but never applied, leaving the store in a self‑inconsistent state that will fail on replay and replication.\n\n**Signals / Evidence**\n- `ingest_remote_batch` appends records + commits index, then calls `apply_event` in a loop (`src/daemon/core.rs`).\n- `apply_event` can fail for `MissingBead`, `NoteCollision`, `InvalidDependency`, etc. (`src/core/apply.rs`).\n- On failure, we return `internal` error but do not roll back WAL/index.\n\n**Why this hurts velocity**\nThis is the kind of “works until it really doesn’t” failure that makes debugging impossible: the durable log and in-memory state diverge. Every operator‑facing repair becomes bespoke.","design":"**Design**\nOption A (preferred): add a validation step that guarantees `apply_event` cannot fail for accepted events. For remote ingest, validate against current state *before* WAL append (or implement an `apply_event_checked` that computes the outcome without side effects).\nOption B: restructure ingestion so WAL append + index commit happen *after* apply, but only if apply succeeds, and treat apply failures as corruption/reject before durability.\nEither way, if an event can’t be applied, it must be rejected *before* it is appended/indexed, with a domain error (`corruption` or `invalid_request`).\n\n**Design Notes**\nIf we stick with Option A, consider a small “preflight” that checks just the known `ApplyError` preconditions (bead existence for note_append, dep key validity, creation stamp consistency), to avoid cloning state.","id":"bd-km0","labels":[],"priority":1,"status":"closed","title":"Prevent WAL/index divergence when apply_event fails during replication ingest","type":"bug"}
{"_at":[1768485481523,1],"_by":"darin@book","_v":{"acceptance_criteria":[[1768444217587,1],"darin@book"],"claim":[[1768483771375,1],"darin@book"],"description":[[1768444217587,1],"darin@book"],"design":[[1768444217587,1],"darin@book"],"estimated_minutes":[[1768444217587,1],"darin@book"],"external_ref":[[1768444217587,1],"darin@book"],"labels":[[1768444217587,1],"darin@book"],"priority":[[1768444217587,1],"darin@book"],"source_repo":[[1768444217587,1],"darin@book"],"title":[[1768444217587,1],"darin@book"],"type":[[1768444217587,1],"darin@book"]},"acceptance_criteria":"- [ ] Legacy snapshot WAL is not initialized in the normal runtime path.\n- [ ] `apply_wal_mutation` is removed or clearly isolated to legacy/migration mode.\n- [ ] No production code depends on legacy WAL once realtime is verified.","assignee":"darin@book","assignee_at":[1768483771375,1],"assignee_expires":1768487371375,"closed_at":[1768485481523,1],"closed_by":"darin@book","created_at":[1768444217587,1],"created_by":"darin@book","description":"**Problem**\nLegacy snapshot WAL (`wal_legacy_snapshot`) and `apply_wal_mutation` are still wired into the daemon runtime even though realtime WAL + replication are in place. The legacy WAL is initialized on daemon start and the code path remains in `Daemon` despite not being used by the new mutation pipeline. This keeps two durability systems alive, increases cognitive load, and makes future changes riskier.\n\n**Signals / Evidence**\n- Daemon still constructs legacy `Wal` in `run_daemon` (`src/daemon/run.rs`).\n- `Daemon` stores `wal: Arc<Wal>` and exposes `apply_wal_mutation` even though new mutations use event WAL (`src/daemon/core.rs`).\n- `apply_wal_mutation` is only referenced in tests (`src/daemon/core.rs`).\n\n**Why this hurts velocity**\nTwo durability paths mean double mental model and duplicated invariants. It’s easy to accidentally keep legacy behavior alive or regress realtime expectations.","design":"**Design**\n- Gate legacy WAL behind a feature flag or explicit migration mode.\n- Remove `apply_wal_mutation` from the main execution path (keep only for legacy migration tests if needed).\n- If realtime is stable, remove the legacy WAL from daemon startup entirely and leave migration in a separate tool/module.\n- Document the deprecation timeline and add a kill switch if we need emergency fallback.","id":"bd-q4z","labels":[],"priority":3,"status":"closed","title":"Deprecate legacy snapshot WAL and remove from runtime path","type":"chore"}
